{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rwAAkyw4-NlS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading plant-disease-recognition-dataset, 1339624018 bytes compressed\n",
      "[==================================================] 1339624018 bytes downloaded\n",
      "Downloaded and uncompressed: plant-disease-recognition-dataset\n",
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
    "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tempfile import NamedTemporaryFile\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import unquote, urlparse\n",
    "from urllib.error import HTTPError\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "CHUNK_SIZE = 40960\n",
    "DATA_SOURCE_MAPPING = 'plant-disease-recognition-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1447507%2F2394131%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240927%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240927T193255Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D353110a6aa8b8dc4f9858164eada952d1c311704ee79d5e45354500665f858ebe05e6cbfda579cd52f2206ce7b536bad6618458f5da269daa87443fe63a15d125b58b3c887e2d92993fb2d7173b2bb9431acde84b3317bf7f142122370b2a360b408f6915392b9409efee2913179067eb54039a927e4193738cc3a829e3b96e5d5a381a2dd7e0c00c7e481385608f7b57692b259efa2c0d4a9085c16d44d76b4b7b5944013f69379f8b18de1a617a3a6843d0a57eb3bd722b8782efc943af4dc5567d1b021fa9af04e05531cfba06b0db6cded8e2a5fee8977713d757af8f5802adb21c477603bb6880742a370c91f82409a2a5ab31843093524091d93342973'\n",
    "\n",
    "KAGGLE_INPUT_PATH='kaggle/input'\n",
    "KAGGLE_WORKING_PATH='kaggle/working'\n",
    "KAGGLE_SYMLINK='kaggle'\n",
    "\n",
    "!umount /kaggle/input/ 2> /dev/null\n",
    "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
    "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
    "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
    "\n",
    "try:\n",
    "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "try:\n",
    "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
    "    directory, download_url_encoded = data_source_mapping.split(':')\n",
    "    download_url = unquote(download_url_encoded)\n",
    "    filename = urlparse(download_url).path\n",
    "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "    try:\n",
    "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
    "            total_length = fileres.headers['content-length']\n",
    "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
    "            dl = 0\n",
    "            data = fileres.read(CHUNK_SIZE)\n",
    "            while len(data) > 0:\n",
    "                dl += len(data)\n",
    "                tfile.write(data)\n",
    "                done = int(50 * dl / int(total_length))\n",
    "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "                sys.stdout.flush()\n",
    "                data = fileres.read(CHUNK_SIZE)\n",
    "            if filename.endswith('.zip'):\n",
    "              with ZipFile(tfile) as zfile:\n",
    "                zfile.extractall(destination_path)\n",
    "            else:\n",
    "              with tarfile.open(tfile.name) as tarfile:\n",
    "                tarfile.extractall(destination_path)\n",
    "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
    "    except HTTPError as e:\n",
    "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "        continue\n",
    "    except OSError as e:\n",
    "        print(f'Failed to load {download_url} to path {destination_path}')\n",
    "        continue\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtvvpz5V-NlV"
   },
   "source": [
    "<div style=\"font-family: Calibri, serif; text-align: center;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 15px solid orange;\n",
    "               width: 100%;\n",
    "               margin-bottom: 20px;\n",
    "               margin-left: 45;\n",
    "               height: 20%\">\n",
    "    <div style=\"font-size: 56px;\"><b>ðŸ§  Convolutional<br>Neural Network<br>From Scratch</b></div><br>\n",
    "        <hr style=\"border: none;\n",
    "               border-top: 15px solid orange;\n",
    "               width: 100%;\n",
    "               margin-bottom: 20px;\n",
    "               margin-left: 45;\n",
    "               height: 20%\"> <br>\n",
    "    <div style=\"font-weight: bold;\n",
    "                text-transform: uppercase;\n",
    "                margin-top: 20px;\n",
    "                letter-spacing: 2.5px;\n",
    "                \">2023 | <a href =\"https://www.kaggle.com/lusfernandotorres/\">Â© Luis Fernando Torres</a></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFBAoHYv-NlW"
   },
   "source": [
    "<div style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 30px;\n",
    "               margin-bottom: 20px;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 16px; letter-spacing: 1.5px;\"><b>Table of Contents</b></div>\n",
    "</div>\n",
    "\n",
    "- [Introduction](#intro)<br><br>\n",
    "    - [Convolutional Layer](#convolution)<br><br>\n",
    "    - [Pooling Layer](#pooling)<br><br>\n",
    "    - [Fully-connected Layer](#fc)<br><br>\n",
    "- [CNNS and Computer Vision](#cv)<br><br>\n",
    "- [This Notebook](#goal)<br><br>\n",
    "- [Exploring the Data](#eda)<br><br>\n",
    "- [Preprocessing](#preprocess)<br><br>\n",
    "- [Data Augmentation](#augmentation)<br><br>\n",
    "- [Building the Convolutional Neural Network](#build)<br><br>\n",
    "- [Validating Performance](#val)<br><br>\n",
    "- [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beIk7RAT-NlW"
   },
   "source": [
    "<div id = 'intro'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>Introduction</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjjrDSkW-NlW"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b><i>Convolutional Neural Networks (CNNs or ConvNets)</i></b> are specialized neural architectures that is predominantly used for several <b>computer vision</b> tasks, such as image classification and object recognition. These neural networks harness the power of <i>Linear Algebra</i>, specifically through convolution operations, to identify patterns within images.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Convolutional neural networks have three main kinds of layers, which are:</p>\n",
    "          \n",
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">â€¢ Convolutional layer</p>\n",
    "          \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">â€¢ Pooling layer</p>\n",
    "          \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">â€¢ Fully-connected layer</p></div>\n",
    "\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The convolutional layer is the first layer of the network, while the fully-connected layer is the final layer, responsible for the output. The first convolutional layer may be followed by several additional convolutional layers or pooling layers; and with each new layer, the more complex is the CNN.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">As the CNN gets more complex, the more it excels in identifying greater portions of the image. Whereas earlier layers focus on the simple features, such as colors and edges; as the image progresses through the network, the CNN starts to recognize larger elements and shapes, until finally reaching its main goal. </p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The image below displays the structure of a CNN. We have an input image, followed by Convolutional and Pooling layers, where the feature learning process happens. Later on, we have the layers responsible for the task of classifying whether the vehicle in the input data is a car, truck, van, bicycle, etc.</p>       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7y3tGIX-NlW"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://miro.medium.com/v2/resize:fit:1358/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg\">\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Image displaying the structure of a Convolutional Neural Networks. <br> Source: <a href = \"https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\">Understanding of Convolutional Neural Network (CNN) â€” Deep Learning</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAPaGegz-NlW"
   },
   "source": [
    "<p id = 'convolution' style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>Convolutional Layer</b></p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The convolutional layer is the most important layer of a CNN; responsible for dealing with the major computations. The convolutional layer includes <b>input data</b>, <b> a filter</b>, and <b>a feature map</b>.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">To illustrate how it works, let's assume we have a color image as input. This image is made up of a matrix of pixels in 3D, representing the three dimensions of the image: height, width, and depth.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The filterâ€”which is also referred to as kernelâ€”is a two-dimensional array of weights, and is typically a $3\\times3$ matrix. It is applied to a specific area of the image, and a <b>dot product</b> is computed between the input pixels and the weights in the filter. Subsequently, the filter shifts by a stride, and this whole process is repeated until the kernel slides through the entire image, resulting in an output array.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The resulting output array is also known as a feature map, activation map, or convolved feature.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9kQXqAc-NlX"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://miro.medium.com/v2/resize:fit:1358/1*D6iRfzDkz-sEzyjYoVZ73w.gif\">\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">GIF displaying the convolutional process. First, we have a $5\\times5$ matrixâ€”pixels in the input imageâ€”with a $3\\times3$ filter. The result of the operation is the output array.<br> Source: <a href = \"https://medium.datadriveninvestor.com/convolutional-neural-networks-3b241a5da51e\">Convolutional Neural Networks</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX7LR4r6-NlX"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">It is important to note that the weights in the filter remain fixed as it moves across the image. The weights values are adjusted during the training process due to backpropagation and gradient descent.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;font-size: 24px; letter-spacing: .85px;\">Besides the weights in the filter, we have other three important parameters that need to be set before the training begins:</p>\n",
    "          \n",
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Number of Filters:</b> This parameter is responsible for defining the depth of the output. If we have three distinct filters, we have three different feature maps, creating a depth of three.</p>\n",
    "          \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Stride:</b> This is the distance, or number of pixels, that the filter moves over the input matrix.  </p>\n",
    "          \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Zero-padding:</b> This parameter is usually used when the filters do not fit the input image. This sets all elements outside the input matrix to zero, producing a larger or equally sized output. There are three different kinds of padding:</p>\n",
    "\n",
    "<div style = \"margin-left: 34px;\">\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â–  Valid padding:</b> Also known as <i>no padding</i>. In this specific case, the last convolution is dropped if the dimensions do not align.</p>\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â–  Same padding:</b> This padding ensures that the output layer has the exact same size as the input layer.</p>\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â–  Full padding:</b> This kind of padding increases the size of the output by adding zeros to the borders of the input matrix.</p></div></div>\n",
    "\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;font-size: 24px; letter-spacing: .85px;\">After each convolution operation, we have the application of a <i><b>Rectified Linear Unit (ReLU)</b></i> function, which transforms the feature map and introduces nonlinearity.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVlDLY4m-NlX"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://www.researchgate.net/profile/Leo-Pauly/publication/319235847/figure/fig3/AS:537056121634820@1505055565670/ReLU-activation-function.png\">\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\"><i>ReLU</i> activation function:<br> $f(u)$ = $\\begin{cases} 0 & \\text{if } u \\leq 0\\\\ u & \\text{if } u > 0 \\end{cases}$<br> <br>Source: <a href = \"https://www.researchgate.net/figure/ReLU-activation-function_fig3_319235847\">ResearchGate</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-kKrbCY-NlX"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;font-size: 24px; letter-spacing: .85px;\">As mentioned earlier, the initial convolutional layer can be followed by additional convolutional layers.</p>\n",
    "\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;font-size: 24px; letter-spacing: .85px;\">The subsequent convolutional layers can see the pixels within the receptive fields of the prior layers, which helps to extract and interpret additional patterns.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs5IBh3S-NlX"
   },
   "source": [
    "<p id = 'pooling' style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>Pooling Layer</b></p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The pooling layer is responsible for reducing the dimensionality of the input. It also slides a filter across the entire inputâ€”without any weightsâ€”to populate the output array. We have two main types of pooling:</p>\n",
    "          \n",
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Max Pooling:</b> As the filter slides through the input, it selects the pixel with the highest value for the output array.</p>\n",
    "\n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Average Pooling:</b> The value selected for the output is obtained by computing the average within the receptive field.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWkpXreF-NlX"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://www.researchgate.net/profile/Imran-Ali-12/publication/340812216/figure/fig4/AS:928590380138496@1598404607456/Pooling-layer-operation-oproaches-1-Pooling-layers-For-the-function-of-decreasing-the.png\">\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Illustration of the pooling process.<br>Source: <a href = \"https://www.researchgate.net/figure/Pooling-layer-operation-oproaches-1-Pooling-layers-For-the-function-of-decreasing-the_fig4_340812216\">ResearchGate</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6aC8b1f-NlX"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The pooling layer serves the purpose of reducing complexity, improving efficiency, and limiting the risk of overfitting.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsk-0Xo3-NlY"
   },
   "source": [
    "<p id = 'fc' style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>Fully-connected Layer</b></p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">This is the layer responsible for performing the task classification based on the features extracted during the previous layers. While both convolutional and pooling layers tend to use $ReLU$ functions, fully-connected layers use the <i><b>Softmax</b></i> activation function for classification, producing a probability from <i>0</i> to <i>1</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6IaaY7L-NlY"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://www.researchgate.net/profile/Binghui-Chen/publication/319121953/figure/fig2/AS:527474636398592@1502771161390/Softmax-activation-function.png\">\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\"><i>Softmax</i> activation function graph.<br>Source: <a href = \"https://www.researchgate.net/figure/Softmax-activation-function_fig2_319121953\">ResearchGate</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y0E5AFI-NlY"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">\n",
    "\\begin{equation}\n",
    "    \\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\\n",
    "    \\end{equation}</p>\n",
    "\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">\n",
    "Where:</p>\n",
    "\n",
    "- <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">\n",
    "$\\sigma{(z_i)}$ = The softmax function applied to the $i^{th}$ element of the input vector. This value ranges between 0 and 1.</p>\n",
    "\n",
    "- <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">\n",
    "$e^{z_i}$ = The exponential function applied to the $i^{th}$ element of the input vector.</p>\n",
    "\n",
    "- <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">\n",
    "$\\sum_{j=1}^K e^{z_{j}}$ = The sum of the exponential of each element in the input vector from to $K$, where $K$ is the total number of classes/labels.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0xxDDJN-NlY"
   },
   "source": [
    "<div id = 'cv'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b><i>CNNs</i> and Computer Vision</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwHrY2XS-NlY"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Due to its power in image recognition tasks, CNNs have been highly effective in many fields related to <i><b>Computer Vision</b></i>.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Computer Vision is a field of AI that enables computers to extract information from digital images, videos, and other visual inputs. Some common applications of computer vision today can be seen across several industries, including the following:</p>\n",
    "          \n",
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Social Media:</b> <i>Google</i>, <i>Meta</i>, and <i>Apple</i> use these systems to identify people in a photograph, making it easier to organize photo albums and tag friends.</p>\n",
    "\n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Healthcare:</b> Computer vision models have been used to help doctors identifying cancerous tumors in patients, as well as other conditions.</p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Agriculture:</b> Drones equipped with cameras can monitor the health of vast farmlands to identify areas that need more water or fertilizers.</p>    \n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Security:</b> Surveillance systems can detect unusual and suspect activities  in real time.</p>     \n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Finance:</b> Computer vision models may be used to identify relevant patterns in candlestick charts to predict price movements.</p>        \n",
    "\n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Automotive:</b> Computer vision is an essential component of the research leading to self-driving cars.</p>   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYmrkGWM-NlY"
   },
   "source": [
    "<div id = 'goal'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>This Notebook</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdVpAIYr-NlY"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Nowadays, there are several <i>pre-trained</i> CNNs available for many tasks. Models like <i><b>ResNet</b></i>, <i><b>VGG16</b></i>, <i><b>InceptionV3</b></i>, as well as many others, are highly efficient in most computer vision tasks we currently perform across industries.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">In this notebook, however, I would like to explore the process of building a simple, yet effective, Convolutional Neural Network from scratch. For this task, I will use <b><i>Keras</i></b> to help us build a neural network that can accurately identify diseases in a plant through images.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">I am going to use the <a href =\"https://www.kaggle.com/datasets/rashikrahmanpritom/plant-disease-recognition-dataset/\">Plant Disease Recognition Dataset</a>, which contains 1,530 images divided into train, test, and validation sets. The images are labeled as <i>â€œHealthyâ€œ</i>, <i>â€œRustâ€œ</i>, and <i>â€œPowderyâ€œ</i> to describe the conditions of the plants.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Very briefly, each class means the following:</p>\n",
    "          \n",
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Rust:</b> These are plant diseases caused by Pucciniales fungi, which cause severe deformities to the plant.</p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Powdery:</b> Powdery mildews are caused by Erysphales fungi, posing a threat to agriculture and horticulture by reducing crop yields.</p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Healthy:</b> Naturally, these are the plants that are free from diseases.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "KLXGiiR5-NlY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Efficient Looping\n",
    "import itertools\n",
    "\n",
    "# Traceback for diagnosis\n",
    "import traceback\n",
    "\n",
    "# Data Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "from IPython.display import display\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Statistics & Mathematics\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import shapiro, skew, anderson, kstest\n",
    "import math\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import (\n",
    "    RFECV, SelectKBest, chi2, f_classif, f_regression,\n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin,ClassifierMixin\n",
    "\n",
    "# Preprocessing data\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, QuantileTransformer, FunctionTransformer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Model Selection for Cross Validation\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, KFold,\n",
    "    RepeatedKFold, RepeatedStratifiedKFold,\n",
    "    train_test_split, TimeSeriesSplit\n",
    ")\n",
    "\n",
    "# Machine Learning metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    cohen_kappa_score,\n",
    "    make_scorer,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ML regressors\n",
    "from sklearn.linear_model import HuberRegressor,RANSACRegressor, TheilSenRegressor, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingRegressor, StackingRegressor,\n",
    "    AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
    "    )\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# ML classifiers\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingClassifier, AdaBoostClassifier,\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    StackingClassifier, VotingClassifier,ExtraTreesClassifier\n",
    "    )\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fine-tuning\n",
    "import optuna\n",
    "\n",
    "# Randomizer\n",
    "import random\n",
    "\n",
    "# Encoder of categorical variables\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# OS\n",
    "import os\n",
    "\n",
    "# Image package\n",
    "from PIL import Image\n",
    "\n",
    "# Hiding warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uN-5L9Ib-NlY"
   },
   "outputs": [],
   "source": [
    "# Importing Keras\n",
    "from keras.models import Sequential                          # Neural network model as a sequence of layers.\n",
    "from keras.layers import Conv2D                              # Convolutional layer\n",
    "from keras.layers import MaxPooling2D                        # Max pooling layer\n",
    "from keras.layers import Flatten                             # Layer used to flatten 2D arrays for fully-connected layers.\n",
    "from keras.layers import Dense                               # This layer adds fully-connected layers to the neural network.\n",
    "from keras.layers import Dropout                             # This serves to prevent overfitting by dropping out a random set of activations.\n",
    "from keras.layers import BatchNormalization                  # This is used to normalize the activations of the neurons.\n",
    "from keras.layers import Activation                          # Layer for activation functions\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint   # Classes used to save weights and stop training when improvements reach a limit\n",
    "from keras.models import load_model                          # This helps us to load trained models\n",
    "# Preprocessing layers\n",
    "from keras.layers import Rescaling                           # This layer rescales pixel values\n",
    "\n",
    "# Importing TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ijQWdTnA-NlY"
   },
   "outputs": [],
   "source": [
    "# Configuring notebook\n",
    "seed = 123\n",
    "paper_color = '#EEF6FF'\n",
    "bg_color = '#EEF6FF'\n",
    "#colormap =\n",
    "#template ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "id": "RUf0xi37-NlY"
   },
   "outputs": [],
   "source": [
    "def image_resizer(paths):\n",
    "    \"\"\"\n",
    "    This function resizes the input images\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        resized_images = list(executor.map(lambda x: Image.open(x).resize((350,250)), paths))\n",
    "    return resized_images\n",
    "\n",
    "def plot_images_list(images, title, subtitle):\n",
    "    '''\n",
    "    This functions helps to plot a matrix of images in a list\n",
    "    '''\n",
    "    fig = sp.make_subplots(rows=3, cols=3)\n",
    "    images = image_resizer(images)\n",
    "\n",
    "    traces = []\n",
    "    for i in range(min(9, len(images))):\n",
    "        img = go.Image(z=images[i])\n",
    "        traces.append((img, i//3+1, i%3+1))\n",
    "\n",
    "    fig.add_traces([trace[0] for trace in traces],\n",
    "                  rows = [trace[1] for trace in traces],\n",
    "                  cols = [trace[2] for trace in traces])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={'text': f'<b>{title}<br>  <i><sub>{subtitle}</sub></i></b>',\n",
    "               'font': dict(size = 22)},\n",
    "        height=800,\n",
    "        width=800,\n",
    "        margin=dict(t=110, l=80),\n",
    "        plot_bgcolor=bg_color,paper_bgcolor=paper_color\n",
    "        #template=template\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "id": "lM76kEN4-NlY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Found! Using GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 14:44:09.243632: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Ultra\n",
      "2024-09-27 14:44:09.243698: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 128.00 GB\n",
      "2024-09-27 14:44:09.243761: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 48.00 GB\n",
      "2024-09-27 14:44:09.243829: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-27 14:44:09.243876: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Configuring GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        print('\\nGPU Found! Using GPU...')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Number of replicas:', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeW41cyg-NlY"
   },
   "source": [
    "<div id = 'eda'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>Exploring the Data</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngbfci1b-NlZ"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Before building our Convolutional Neural Network, it is helpful to perform a brief, yet efficient, analysis of the data we have at hand. Let's start by loading the directories for each set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FYvLaWdB-NlZ"
   },
   "outputs": [],
   "source": [
    "# Loading training, testing, and validation directories\n",
    "train_dir = 'kaggle/input/plant-disease-recognition-dataset/Train/Train'\n",
    "test_dir = 'kaggle/input/plant-disease-recognition-dataset/Test/Test'\n",
    "val_dir = 'kaggle/input/plant-disease-recognition-dataset/Validation/Validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQnAQgy2-NlZ"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We may also count the files inside each subfolder to compute the total of data we have for training and testing, as well as measure the degree of class imbalance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "id": "zjYwOEca-NlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * * * Number of files in each folder * * * * *\n",
      "\n",
      "\n",
      "Train/Healthy: 458\n",
      "\n",
      "Train/Powdery: 430\n",
      "\n",
      "Train/Rust: 434\n",
      "\n",
      "  Total: 1322\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test/Healthy: 50\n",
      "\n",
      "Test/Powdery: 50\n",
      "\n",
      "Test/Rust: 50\n",
      "\n",
      "  Total: 150\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Validation/Healthy: 20\n",
      "\n",
      "Validation/Powdery: 20\n",
      "\n",
      "Validation/Rust: 20\n",
      "\n",
      "  Total: 60\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Giving names to each directory\n",
    "directories = {\n",
    "    train_dir: 'Train',\n",
    "    test_dir: 'Test',\n",
    "    val_dir: 'Validation'\n",
    "    }\n",
    "\n",
    "# Naming subfolders\n",
    "subfolders = ['Healthy', 'Powdery', 'Rust']\n",
    "\n",
    "print('\\n* * * * * Number of files in each folder * * * * *\\n')\n",
    "\n",
    "# Counting the total of pictures inside each subfolder and directory\n",
    "for dir, name in directories.items():\n",
    "    total = 0\n",
    "    for sub in subfolders:\n",
    "        path = os.path.join(dir, sub)\n",
    "        num_files = len([f for f in os.listdir(path) if os.path.join(path, f)])\n",
    "        total += num_files\n",
    "        print(f'\\n{name}/{sub}: {num_files}')\n",
    "    print(f'\\n  Total: {total}')\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU37nJqx-NlZ"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We have a total of <b>1,322</b> files inside the <code>Train</code> directory and there are no large imbalances between classes. A small variation between them is fine, and a simple metric such as <i>Accuracy</i> may be enough to measure performance.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">For the testing set, we have a total of <b>150</b> images, whereas the validation set consists of <b>60</b> images in total. Both sets have a perfect class balance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxK5ANdN-NlZ"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Convolutional Neural Networks require a fixed size for all images we feed into it. This means that every single image in our dataset must be equally sized, either $128 \\times 128$, $224 \\times 224$, and so on.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We can also check if our data meets this requirement, or if it will be necessary to perform some preprocessing in this regard before modeling.</p>          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "id": "5lQ1qsHG-NlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 8 unique image dimensions: {(4032, 3024), (4000, 2672), (4000, 3000), (5184, 3456), (2592, 1728), (3901, 2607), (4608, 3456), (2421, 2279)}\n"
     ]
    }
   ],
   "source": [
    "unique_dimensions = set()\n",
    "\n",
    "for dir, name in directories.items():\n",
    "    for sub in subfolders:\n",
    "        folder_path = os.path.join(dir, sub)\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            with Image.open(image_path) as img:\n",
    "                unique_dimensions.add(img.size)\n",
    "\n",
    "if len(unique_dimensions) == 1:\n",
    "    print(f\"\\nAll images have the same dimensions: {unique_dimensions.pop()}\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(unique_dimensions)} unique image dimensions: {unique_dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mUxP3ys-NlZ"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We have 8 different dimensions across the dataset. In the next cell, I am going to check the distribution of these dimensions across the data.</p>         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "id": "23KCPEbT-NlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimension (4000, 2672): 1130 images\n",
      "\n",
      "Dimension (4000, 3000): 88 images\n",
      "\n",
      "Dimension (2421, 2279): 1 images\n",
      "\n",
      "Dimension (4608, 3456): 72 images\n",
      "\n",
      "Dimension (2592, 1728): 127 images\n",
      "\n",
      "Dimension (5184, 3456): 97 images\n",
      "\n",
      "Dimension (4032, 3024): 16 images\n",
      "\n",
      "Dimension (3901, 2607): 1 images\n"
     ]
    }
   ],
   "source": [
    "# Checking if all the images in the dataset have the same dimensions\n",
    "dims_counts = defaultdict(int)\n",
    "\n",
    "for dir, name in directories.items():\n",
    "    for sub in subfolders:\n",
    "        folder_path = os.path.join(dir, sub)\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            with Image.open(image_path) as img:\n",
    "                dims_counts[img.size] += 1\n",
    "\n",
    "for dimension, count in dims_counts.items():\n",
    "    print(f\"\\nDimension {dimension}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40xvZzBP-NlZ"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">It seems that most images have dimensions of $4000 \\times 2672$, which is a <b>rectangular shape</b>. We can conclude that, due to the differences in dimensions, we will need to apply some preprocessing to the data.</p>         \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">First, we are going to resize the images, so they all have the same shape. Then, we will transform the input from rectangular shape to <b>square</b> shape.</p>         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HUGTl39-NlZ"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Another crucial consideration is verifying the pixel valye range of the images. In this case, all images should have pixel values spanning from <b>0</b> to <b>255</b>. This consistency simplifies the preprocessing step, since we often normalize pixel values in images to a range going from 0 to 1.</p>         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "id": "1Ki6hpMF-NlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Not all images are of data type uint8\n",
      "\n",
      " - All images have pixel values ranging from 0 to 255\n"
     ]
    }
   ],
   "source": [
    "# Checking images dtype\n",
    "all_uint8 = True\n",
    "all_in_range = True\n",
    "\n",
    "for dir, name in directories.items():\n",
    "    for sub in subfolders:\n",
    "        folder_path = os.path.join(dir, sub)\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            with Image.open(image_path) as img:\n",
    "                img_array = np.array(img)\n",
    "\n",
    "            if img_array.dtype == 'uint8':\n",
    "                all_uint8 = False\n",
    "\n",
    "            if img_array.min() < 0 or img_array.max() > 255:\n",
    "                all_in_range = False\n",
    "\n",
    "if all_uint8:\n",
    "    print(\" - All images are of data type uint8\\n\")\n",
    "else:\n",
    "    print(\" - Not all images are of data type uint8\\n\")\n",
    "\n",
    "if all_in_range:\n",
    "    print(\" - All images have pixel values ranging from 0 to 255\")\n",
    "else:\n",
    "    print(\" - Not all images have the same pixel values from 0 to 255\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNvpLQtW-Nld"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Even though not all images are of the same data type, <code>uint8</code>, it is fairly easy to guarantee that they will have the same data type once we load images into datasets. We confirmed, though, that all the images have pixel values ranging from 0 to 255, which is great news.</p>         \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Before moving on to the <i>Preprocessing</i> step, let's plot some images from each class to see what they look like.</p>         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "D7Dg5TdW-Nld"
   },
   "outputs": [],
   "source": [
    "# Loading the directory for each class in the training dataset\n",
    "train_healthy_dir = train_dir + \"/\" + 'Healthy'\n",
    "train_rust_dir = train_dir + \"/\" + 'Rust'\n",
    "train_powdery_dir = train_dir + \"/\" + 'Powdery'\n",
    "\n",
    "# Selecting 9 random pictures from each directory\n",
    "healthy_files = random.sample(os.listdir(train_healthy_dir), 9)\n",
    "rust_files = random.sample(os.listdir(train_rust_dir), 9)\n",
    "powdery_files = random.sample(os.listdir(train_powdery_dir), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-input": true,
    "id": "JSC6HFCk-Nld"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting healthy plants\u001b[39;00m\n\u001b[1;32m      2\u001b[0m healthy_images \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_healthy_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m healthy_files]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplot_images_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhealthy_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHealthy Plants\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mplot_images_list\u001b[0;34m(images, title, subtitle)\u001b[0m\n\u001b[1;32m     21\u001b[0m fig\u001b[38;5;241m.\u001b[39madd_traces([trace[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces],\n\u001b[1;32m     22\u001b[0m               rows \u001b[38;5;241m=\u001b[39m [trace[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces],\n\u001b[1;32m     23\u001b[0m               cols \u001b[38;5;241m=\u001b[39m [trace[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces])\n\u001b[1;32m     25\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(\n\u001b[1;32m     26\u001b[0m     title\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<b>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<br>  <i><sub>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</sub></i></b>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mdict\u001b[39m(size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m22\u001b[39m)},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#template=template\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/MLExp/lib/python3.10/site-packages/plotly/basedatatypes.py:3410\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/MLExp/lib/python3.10/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Plotting healthy plants\n",
    "healthy_images = [os.path.join(train_healthy_dir, f) for f in healthy_files]\n",
    "plot_images_list(healthy_images, \"Healthy Plants\", \"Training Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "DzFH576L-Nld"
   },
   "outputs": [],
   "source": [
    "# Plotting rust plants\n",
    "rust_images = [os.path.join(train_rust_dir, f) for f in rust_files]\n",
    "plot_images_list(rust_images, \"Rust Plants\", \"Training Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "xtBUh-rI-Nld"
   },
   "outputs": [],
   "source": [
    "# Plotting powdery plants\n",
    "powdery_images = [os.path.join(train_powdery_dir, f) for f in powdery_files]\n",
    "plot_images_list(powdery_images, \"Powdery Plants\", \"Training Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlXUC-6Z-Nld"
   },
   "source": [
    "<div id = 'preprocess'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>Preprocessing</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr4Y7jb8-Nld"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">For those familiar with tabular data, preprocessing is probably one of the most daunting steps of dealing with neural networks and unstructured data.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">This task can be fairly easy by using TensorFlow's <code>image_dataset_from_directory</code>, which loads images from the directories as a <b>TensorFlow Dataset</b>. This resulting dataset can be manipulated for batching, shuffling, augmentating, and several other preprocessing steps. </p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">I suggest you check <a href = \"https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory\">this link</a> for more information on the <code>image_dataset_from_directory</code> function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Bd31iUM5-Nld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1322 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a Dataset for the Training data\n",
    "train = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,  # Directory where the Training images are located\n",
    "    labels = 'inferred', # Classes will be inferred according to the structure of the directory\n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['Healthy', 'Powdery', 'Rust'],\n",
    "    batch_size = 16,    # Number of processed samples before updating the model's weights\n",
    "    image_size = (256, 256), # Defining a fixed dimension for all images\n",
    "    shuffle = True,  # Shuffling data\n",
    "    seed = seed,  # Random seed for shuffling and transformations\n",
    "    validation_split = 0, # We don't need to create a validation set from the training set\n",
    "    crop_to_aspect_ratio = True # Resize images without aspect ratio distortion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Zf9fuWPw-Nld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset for the Test data\n",
    "test = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['Healthy', 'Powdery', 'Rust'],\n",
    "    batch_size = 16,\n",
    "    image_size = (256, 256),\n",
    "    shuffle = True,\n",
    "    seed = seed,\n",
    "    validation_split = 0,\n",
    "    crop_to_aspect_ratio = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "O31DzBWV-Nld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset for the Test data\n",
    "validation = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['Healthy', 'Powdery', 'Rust'],\n",
    "    batch_size = 16,\n",
    "    image_size = (256, 256),\n",
    "    shuffle = True,\n",
    "    seed = seed,\n",
    "    validation_split = 0,\n",
    "    crop_to_aspect_ratio = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-8nIXxU-Nld"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We have successfully captured all files within each set for each of the three classes. We can also print these datasets for a further understanding of their structure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_kg_hide-input": true,
    "id": "3S22L_kx-Nld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n",
      "\n",
      "Testing Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n",
      "\n",
      "Validation Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining Dataset:', train)\n",
    "print('\\nTesting Dataset:', test)\n",
    "print('\\nValidation Dataset:', validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lro99zuV-Nld"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Let's explore a bit deeper what all the information above means.</p>\n",
    "          \n",
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ _BatchDataset:</b> It indicates that the dataset returns data in batches.</p>\n",
    "    \n",
    "   <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ element_spec:</b> This describes the structure of the elements in the dataset.</p>\n",
    "\n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name = None):</b> This represents the features, in this case the images, in the dataset. <code>None</code> represents the batch size, which is <i>None</i> here because it can vary depending on how many samples we have in the last batch; <code>256, 256</code> represents the height and width of the images; <code>3</code> is the number of channels in the images, indicating they are RGB images. Last, <code>dtype=tf.float32</code> tells us that the data type of the image pixels is a 32-bit floating point.</p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\"><b>â€¢ TensorSpec(shape=(None, 3), dtype=tf.float32, name=None):</b> This represents the labels/targets of our dataset. Here, <code>None</code> refers to the batch size; <code>3</code> refers to the number of labels in the dataset; whilst <code>dtype=tf.float32</code> is also a 32-bit floating point.</p>\n",
    "</div>\n",
    "\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">By using the <code>image_dataset_from_directory</code> function, we have been able to automatically preprocess some aspects of the data. For instance, all the images are now of the same data type, <code>tf.float32</code>. By setting <code>image_size = (256, 256)</code>, we have ensured that all images have the same dimensions, $256 \\times 256$.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esh4Ed4S-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Another important step for preprocessing is ensuring that the pixel values of our images are within a 0 to 1 range. The <code>image_dataset_from_directory</code> method performed some transformations already, but the pixel values are still in the 0 to 255 range.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_kg_hide-input": true,
    "id": "m368OxYc-Nle"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum pixel value in the Validation dataset 0\n",
      "\n",
      "Maximum pixel value in the Validation dataset 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 14:49:57.670300: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Checking minimum and maximum pixel values in the Validation dataset\n",
    "min_value = float('inf')\n",
    "max_value = -float('inf')\n",
    "\n",
    "for img, label in validation:\n",
    "    batch_min = tf.reduce_min(img)\n",
    "    batch_max = tf.reduce_max(img)\n",
    "\n",
    "    min_value = min(min_value, batch_min.numpy())\n",
    "    max_value = max(max_value, batch_max.numpy())\n",
    "\n",
    "print('\\nMinimum pixel value in the Validation dataset', min_value)\n",
    "print('\\nMaximum pixel value in the Validation dataset', max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH423qy3-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">To bring the pixel values to the 0 to 1 range, we can easily use one of Keras' preprocessing layers, <code>tf.keras.layers.Rescaling</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ulyI9mGU-Nle"
   },
   "outputs": [],
   "source": [
    "scaler = Rescaling(1./255) # Defining scaler values between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qsJkncy8-Nle"
   },
   "outputs": [],
   "source": [
    "# Rescaling datasets\n",
    "train = train.map(lambda x, y: (scaler(x), y))\n",
    "test = test.map(lambda x, y: (scaler(x), y))\n",
    "validation = validation.map(lambda x, y: (scaler(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfK0OYs_-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Now we can once more visualize the minimum and maximum pixel values in the validation set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_kg_hide-input": true,
    "id": "X4BjHy6U-Nle"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum pixel value in the Validation dataset 0.0\n",
      "\n",
      "Maximum pixel value in the Validation dataset 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 14:50:25.045885: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Checking minimum and maximum pixel values in the Validation dataset\n",
    "min_value = float('inf')\n",
    "max_value = -float('inf')\n",
    "\n",
    "for img, label in validation:\n",
    "    batch_min = tf.reduce_min(img)\n",
    "    batch_max = tf.reduce_max(img)\n",
    "\n",
    "    min_value = min(min_value, batch_min.numpy())\n",
    "    max_value = max(max_value, batch_max.numpy())\n",
    "\n",
    "print('\\nMinimum pixel value in the Validation dataset', min_value)\n",
    "print('\\nMaximum pixel value in the Validation dataset', max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0YmdBHf-Nle"
   },
   "source": [
    "<div id = 'augmentation'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>Data Augmentation</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIaMqxz0-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">When working with image data, it is usually a good practice to artificially introduce some diversity to the sample by applying random transformations to the images used in training. This is good because it helps to expose the model to a wider variety of images and avoids overfitting.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Keras has about seven different layers for image data augmentation. These are:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lieZA5r1-Nle"
   },
   "source": [
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ <a href =\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_crop#randomcrop-class\">tf.keras.layers.RandomCrop</a></b>: This layer randomly chooses a location to crop images down to a target size. </p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ <a href =\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_flip/\">tf.keras.layers.RandomFlip</a></b>: This layer randomly flips images horizontally and or vertically based on the <code>mode</code> attribute.</p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ <a href =\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_translation/\">tf.keras.layers.RandomTranslation</a></b>: This layer randomly applies translations to each image during training according to the <code>fill_mode</code> attribute.</p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ <a href =\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_brightness/\">tf.keras.layers.RandomBrightness</a></b>: This layer randomly increases/reduces the brightness for the input RGB images. </p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ <a href =\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_rotation/\">tf.keras.layers.RandomRotation</a></b>: This layer randomly rotates the images during training, and also fills empty spaces according to the <code>fill_mode</code> attribute. </p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ <a href =\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_zoom/\">tf.keras.layers.RandomZoom</a></b>: This layer randomly zooms in or out on each axis of each image independently during training. </p>\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ <a href =\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_contrast/\">tf.keras.layers.RandomContrast</a></b>: This layer randomly adjusts contrast by a random factor during training in or out on each axis of each image independently during training. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWBrTrSu-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">For this task, I am going to apply <code>RandomRotation</code>, <code>RandomContrast</code>, as well as <code>RandomBrightness</code> to our images.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "9kSir9SZ-Nle"
   },
   "outputs": [],
   "source": [
    "# Creating data augmentation pipeline\n",
    "augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomRotation(\n",
    "        factor = (-.25, .3),\n",
    "        fill_mode = 'reflect',\n",
    "        interpolation = 'bilinear',\n",
    "        seed = seed),\n",
    "\n",
    "\n",
    "        tf.keras.layers.RandomBrightness(\n",
    "        factor = (-.45, .45),\n",
    "        value_range = (0.0, 1.0),\n",
    "        seed = seed),\n",
    "\n",
    "        tf.keras.layers.RandomContrast(\n",
    "        factor = (.5),\n",
    "        seed = seed)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrY3gNM2-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We can also use an <code>input_shape</code> as example to build the pipeline above and plot it below to illustrate how it looks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_kg_hide-input": true,
    "id": "u8dLY_KI-Nle"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "augmentation.build((None, 256, 256, 3)) # Building model\n",
    "# Plotting model\n",
    "tf.keras.utils.plot_model(augmentation,\n",
    "                          show_shapes = True,\n",
    "                          show_layer_names = True,\n",
    "                          expand_nested = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hiSQEy9-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We are going to attach this data augmentation pipeline to our convolutional neural network. It is important to remember that the data augmentation pipeline is inactive during testing, and the input samples will only be augmented during <code>fit()</code>, not when calling <code>predict()</code>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIa74Pi_-Nle"
   },
   "source": [
    "<div id = 'build'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>Building the Convolutional Neural Network</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-zqgARt-Nle"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">To build the Convolutional Neural Network with Keras, we are going to use the <code>Sequential</code> class. This class allows us to build a linear stack of layers, which is essential for the creation of neural networks.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Besides the Convolutional, Pooling, and Fully-Connected Layers, which we have previously explored, I am also going to add the following layers to the network:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxowV1Ve-Nlf"
   },
   "source": [
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ BatchNormalization</b>: This layer applies a transformation that maintains the mean output close to $0$ and the standard deviation close to $1$. It normalizes its inputs and is important to help convergence and generalization.</p>\n",
    "    \n",
    "   <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Dropout</b>: This layer randomly sets a fraction of input units to $0$ during training, which helps to prevent overfitting.</p>\n",
    "\n",
    "   <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ Flatten</b>: This layer transforms a multi-dimensional tensor into a one-dimensional tensor. It is used when transitioning from the <b>Feature Learning</b> segment â€” Convolutional and Pooling layers â€” to the fully-connected layers.</p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60lUOCkr-Nlf"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">I plan to use different kernel sizes, both $3 \\times 3$ and $5 \\times 5$. This may allow the network to capture features at multiple scales.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">I am also gradually increasing the <i>dropout rates</i> as we advance through the process and the increase in the number of kernels.</p>\n",
    "\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">With that being said, let's go ahead and build our ConvNet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "c8XfF33z-Nlf"
   },
   "outputs": [],
   "source": [
    "# Initiating model on GPU\n",
    "with strategy.scope():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(augmentation) # Adding data augmentation pipeline to the model\n",
    "\n",
    "    # Feature Learning Layers\n",
    "    model.add(Conv2D(32,                  # Number of filters/Kernels\n",
    "                     (3,3),               # Size of kernels (3x3 matrix)\n",
    "                     strides = 1,         # Step size for sliding the kernel across the input (1 pixel at a time).\n",
    "                     padding = 'same',    # 'Same' ensures that the output feature map has the same dimensions as the input by padding zeros around the input.\n",
    "                    input_shape = (256,256,3) # Input image shape\n",
    "                    ))\n",
    "    model.add(Activation('relu'))# Activation function\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (5,5), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (5,5), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(512, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Flattening tensors\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully-Connected Layers\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(3, activation = 'softmax')) # Classification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaBKiSrX-Nlf"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">By using Keras' <code>compile</code> method, we can prepare our neural network for training. This method has several parameters, the ones we will be focusing here are: </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnAzIS19-Nlf"
   },
   "source": [
    "<div style = \"margin-left: 25px;\">\n",
    "    \n",
    "  <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ optimizer</b>: In this parameter, we define the algorithms to adjust the weight updates. This is an important parameter, because choosing the right optimizer is essential to speed convergence. We are going to use <code>RMSprop</code>, which is the best optimizer I've found during the tests I ran.</p>\n",
    "    \n",
    "   <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ loss</b>: This is the loss function we're trying to minimize during training. In this case, we are using <code>categorical_crossentropy</code>, which is a good choice for classification tasks with over two classes.</p>\n",
    "\n",
    "   <p style=\"font-family: Calibri, serif; text-align: left;\n",
    "    font-size: 24px; letter-spacing: .85px;\"><b>â€¢ metrics</b>: This parameter defines the metric that will be used to evaluate performance during training and validation. Since our data is not heavily unbalanced, we may use <code>accuracy</code> for this, which is a very straightforward metric given by the following formula: </p>\n",
    "    <p style=\"font-family: Calibri, serif; text-align: left;font-size: 24px; letter-spacing: .85px;\">\n",
    "    \\begin{equation}\n",
    "    \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "    \\end{equation}\n",
    "    </p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "JCFMzg2N-Nlf"
   },
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(0.0001), # 1e-4\n",
    "              loss = 'categorical_crossentropy', # Ideal for multiclass tasks\n",
    "              metrics = ['accuracy']) # Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTftIlNQ-Nlf"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">After compiling the model, I am going to define an <b>Early Stopping</b> and a <b>Model Checkpoint</b>.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Early Stopping serves the purpose of interrupting the training process when a certain metric stops improving over a period of time. In this case, I am going to configure the <code>EarlyStopping</code> method to monitor the accuracy in the test set, and stop the training process if we don't have any improvement on it after 5 epochs.</p>\n",
    "\n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Model Checkpoint will ensure that only the best weights get saved, and we're also going to define the <i>best weights</i> according to the accuracy of the model in the test set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Y7yjXmgG-Nlf"
   },
   "outputs": [],
   "source": [
    "# Defining an Early Stopping and Model Checkpoints\n",
    "early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
    "                              patience = 5, mode = 'max',\n",
    "                              restore_best_weights = True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.keras',\n",
    "                            monitor = 'val_accuracy',\n",
    "                            save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz0JuTt3-Nlf"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We may now use <code>model.fit()</code> to start the training and testing process.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_kg_hide-input": true,
    "id": "sfHNjEEv-Nlf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 14:51:28.266884: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-09-27 14:51:28.424876: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: CANCELLED: GetNextFromShard was cancelled\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]] [type.googleapis.com/tensorflow.DerivedStatus='']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Mixing different tf.distribute.Strategy objects: <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x387205360> is not <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x3ead0bd90>\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing Model\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train, epochs = 50,\n",
    "        validation_data = test,\n",
    "        callbacks = [early_stopping, checkpoint])\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Bihvle-Nlf"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The highest accuracy for the testing set has been reached at the 22<sup>nd</sup> epoch at 0.9600, or 96%, and didn't improve after that.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">With the <code>history</code> object, we can plot two lineplots showing both the loss function and accuracy for both sets over epochs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T14:01:38.413405Z",
     "iopub.status.busy": "2023-10-15T14:01:38.413024Z",
     "iopub.status.idle": "2023-10-15T14:01:38.456542Z",
     "shell.execute_reply": "2023-10-15T14:01:38.455701Z",
     "shell.execute_reply.started": "2023-10-15T14:01:38.413377Z"
    },
    "id": "xF_TzHYS-Nlf"
   },
   "outputs": [],
   "source": [
    "# Creating subplot\n",
    "fig = make_subplots(rows=1,\n",
    "                    cols=2,\n",
    "                    subplot_titles=['<b>Loss Over Epochs</b>', '<b>Accuracy Over Epochs</b>'],\n",
    "                    horizontal_spacing=0.2)\n",
    "\n",
    "# Loss over epochs\n",
    "train_loss = go.Scatter(x=list(range(len(history.history['loss']))),\n",
    "                        y=history.history['loss'],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='rgba(0, 67, 162, .75)', width=4.75),\n",
    "                        name='Training',\n",
    "                        showlegend = False)\n",
    "\n",
    "val_loss = go.Scatter(x=list(range(len(history.history['val_loss']))),\n",
    "                      y=history.history['val_loss'],\n",
    "                      mode='lines',\n",
    "                      line=dict(color='rgba(255, 132, 0, .75)', width=4.75),\n",
    "                      name='Test',\n",
    "                      showlegend = False)\n",
    "\n",
    "\n",
    "fig.add_trace(train_loss, row=1, col=1)\n",
    "fig.add_trace(val_loss, row=1, col=1)\n",
    "\n",
    "# Accuray over epochs\n",
    "train_acc = go.Scatter(x=list(range(len(history.history['accuracy']))),\n",
    "                       y=history.history['accuracy'],\n",
    "                       mode='lines',\n",
    "                       line=dict(color='rgba(0, 67, 162, .75)', width=4.75),\n",
    "                       name='Training',\n",
    "                       showlegend = True)\n",
    "\n",
    "val_acc = go.Scatter(x=list(range(len(history.history['val_accuracy']))),\n",
    "                     y=history.history['val_accuracy'],\n",
    "                     mode='lines',\n",
    "                     line=dict(color='rgba(255, 132, 0, .75)', width=4.75),\n",
    "                     name='Test',\n",
    "                     showlegend = True)\n",
    "\n",
    "\n",
    "fig.add_trace(train_acc, row=1, col=2)\n",
    "fig.add_trace(val_acc, row=1, col=2)\n",
    "\n",
    "# Updating layout\n",
    "fig.update_layout(\n",
    "    title={'text': '<b>Loss and Accuracy Over Epochs</b>', 'x': 0.025, 'xanchor': 'left'},\n",
    "    margin=dict(t=100),\n",
    "    plot_bgcolor=bg_color,paper_bgcolor=paper_color,\n",
    "    height=500, width=1000,\n",
    "    showlegend= True\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text = 'Loss', row = 1, col = 1)\n",
    "fig.update_yaxes(title_text = 'Accuracy', row = 1, col = 2)\n",
    "\n",
    "fig.update_xaxes(title_text = 'Epoch', row = 1, col = 1)\n",
    "fig.update_xaxes(title_text = 'Epoch', row = 1, col = 2)\n",
    "\n",
    "# Showing figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXE885Wx-Nlf"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">It is possible to see that the loss of the training set decreases continuously over epochs, whereas its accuracy increases. This happens because, at each epoch, the model starts to become more and more aware of the training set's patterns and particularities.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">For the test set, however, this process is a bit more slower. Overall, the lowest loss for the test set happened at epoch 14 at 0.5319, while the accuracy was at its peak at epoch 22, at 0.9600.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Now that our model is built, trained, and tested, we can also plot its architecture, as well as summary to better understand it.</p>          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:31.348694Z",
     "iopub.status.busy": "2023-10-15T13:56:31.347977Z",
     "iopub.status.idle": "2023-10-15T13:56:31.564662Z",
     "shell.execute_reply": "2023-10-15T13:56:31.56373Z",
     "shell.execute_reply.started": "2023-10-15T13:56:31.348662Z"
    },
    "id": "urbQLVdb-Nlf"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model) # Plotting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjtM4Mtr-Nlf"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">In the image, it is possible to visualize the sequential process of the Convolutional Neural Network. First we have a 2D Convolutional Layer, with <i><b>ReLU</b></i> activation function, followed by a BatchNormalization Layer and then a MaxPooling 2D Layer. Finally, we have a Dropout Layer to avoid overfitting. This same pattern repeats a few times until we reach the Flatten Layer, which connects the output of the Feature Learning process to the Dense Layers for the final classification task.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Using <code>model.summary()</code>, we can extract some extra info on the neural network.</p>         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:31.566589Z",
     "iopub.status.busy": "2023-10-15T13:56:31.566007Z",
     "iopub.status.idle": "2023-10-15T13:56:31.629284Z",
     "shell.execute_reply": "2023-10-15T13:56:31.628643Z",
     "shell.execute_reply.started": "2023-10-15T13:56:31.566561Z"
    },
    "id": "HQpf6G-S-Nlf"
   },
   "outputs": [],
   "source": [
    "model.summary() # Printing model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkBJ0A9c-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The summary displays the output shapes for each layer, as well as the number of parameters. We can clearly see, for instance, that the output shape for the first layer is <code>(None, 256,256,3)</code> where $256$ represents both height and width, while $3$ represents the RGB color. In the last dense layer, however, the output shape is <code>(None, 3)</code>, where $3$ represents the three classes for classification.</p>\n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">We can also see that the model has over 69 million parameters, where 99.99% of them are trainable. The non-trainable parameters are the ones from the BatchNormalization layers.</p>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4--7O1e-Nlg"
   },
   "source": [
    "<div id = 'val'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>Validating Performance</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4viu4PiI-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">After finishing the training and testing phase, we may go ahead and validate our model on the validation set. To load the best weights achieved during training, we simply use the <code>load_weights</code> method. These weights will be saved with the same name we've given during the <code>ModelCheckpoint</code> configuration, when we set <code>ModelCheckpoint('best_model.h5')</code>.</p>     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:31.630369Z",
     "iopub.status.busy": "2023-10-15T13:56:31.63007Z",
     "iopub.status.idle": "2023-10-15T13:56:32.22102Z",
     "shell.execute_reply": "2023-10-15T13:56:32.220095Z",
     "shell.execute_reply.started": "2023-10-15T13:56:31.630343Z"
    },
    "id": "qfoNdaQd-Nlg"
   },
   "outputs": [],
   "source": [
    "# Loading best weights\n",
    "model.load_weights('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:32.223174Z",
     "iopub.status.busy": "2023-10-15T13:56:32.222514Z",
     "iopub.status.idle": "2023-10-15T13:56:38.84721Z",
     "shell.execute_reply": "2023-10-15T13:56:38.846175Z",
     "shell.execute_reply.started": "2023-10-15T13:56:32.223141Z"
    },
    "id": "-joRZYk1-Nlg"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(validation)  # Running model on the validation dataset\n",
    "val_loss, val_acc = model.evaluate(validation) # Obtaining Loss and Accuracy on the val dataset\n",
    "\n",
    "print('\\nValidation Loss: ', val_loss)\n",
    "print('\\nValidation Accuracy: ', np.round(val_acc * 100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3r74a7_-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The output for <code>model.predict()</code> consists of probabilities for each class, while <code>model.evaluate()</code> returns loss and accuracy values.</p>  \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">It is clear that the model correctly predicts $97$% of the labels of the images in the validation set.</p>  \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">I am going to load some images from the validation test and run predictions on them individually, so we can see how the model performs according to each picture.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:38.848951Z",
     "iopub.status.busy": "2023-10-15T13:56:38.848616Z",
     "iopub.status.idle": "2023-10-15T13:56:39.272735Z",
     "shell.execute_reply": "2023-10-15T13:56:39.271939Z",
     "shell.execute_reply.started": "2023-10-15T13:56:38.848921Z"
    },
    "id": "9nMGFBl4-Nlg"
   },
   "outputs": [],
   "source": [
    "# Loading an image from the Validation/ Powdery directory\n",
    "image_path = '/kaggle/input/plant-disease-recognition-dataset/Validation/Validation/Powdery/9b6a318cc5721d73.jpg'\n",
    "original_image = Image.open(image_path)\n",
    "og_width, og_height = original_image.size\n",
    "\n",
    "# Resizing image for optimal performance\n",
    "new_width = int(og_width * .20) # 20% of the original size\n",
    "new_height = int(og_height * .20) # 20% of the original size\n",
    "\n",
    "resized_img = original_image.resize((new_width, new_height))\n",
    "print('Picture of a Powdery Plant: \\n')\n",
    "resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:39.274985Z",
     "iopub.status.busy": "2023-10-15T13:56:39.274386Z",
     "iopub.status.idle": "2023-10-15T13:56:39.733431Z",
     "shell.execute_reply": "2023-10-15T13:56:39.732476Z",
     "shell.execute_reply.started": "2023-10-15T13:56:39.274952Z"
    },
    "id": "8RiJ-04a-Nlg"
   },
   "outputs": [],
   "source": [
    "# Manually preprocessing image\n",
    "preprocessed_image = original_image.resize((256, 256))\n",
    "preprocessed_image = np.array(preprocessed_image) / 255.0\n",
    "\n",
    "preds = model.predict(np.expand_dims(preprocessed_image, axis = 0))\n",
    "labels = ['Healthy', 'Powdery', 'Rust']\n",
    "\n",
    "preds_class = np.argmax(preds)\n",
    "preds_label = labels[preds_class]\n",
    "\n",
    "print(f'\\nPredicted Class: {preds_label}')\n",
    "print(f'\\nConfidence Score: {preds[0][preds_class]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfTgtulz-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The model is about 99.9% confident that the plant in the picture belongs to the <i><b>Powdery</b></i> class, which is correct.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:39.735632Z",
     "iopub.status.busy": "2023-10-15T13:56:39.734629Z",
     "iopub.status.idle": "2023-10-15T13:56:40.101732Z",
     "shell.execute_reply": "2023-10-15T13:56:40.100984Z",
     "shell.execute_reply.started": "2023-10-15T13:56:39.735598Z"
    },
    "id": "rRUL0GOe-Nlg"
   },
   "outputs": [],
   "source": [
    "# Loading an image from the Validation/ Rust directory\n",
    "image_path = '/kaggle/input/plant-disease-recognition-dataset/Validation/Validation/Rust/8152cfbd5a28b5d2.jpg'\n",
    "original_image = Image.open(image_path)\n",
    "og_width, og_height = original_image.size\n",
    "\n",
    "# Resizing image for optimal performance\n",
    "new_width = int(og_width * .20) # 20% of the original size\n",
    "new_height = int(og_height * .20) # 20% of the original size\n",
    "\n",
    "resized_img = original_image.resize((new_width, new_height))\n",
    "print('Picture of a Rust Plant: \\n')\n",
    "resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:40.103557Z",
     "iopub.status.busy": "2023-10-15T13:56:40.102798Z",
     "iopub.status.idle": "2023-10-15T13:56:40.32906Z",
     "shell.execute_reply": "2023-10-15T13:56:40.328169Z",
     "shell.execute_reply.started": "2023-10-15T13:56:40.103525Z"
    },
    "id": "GYhPQGS9-Nlg"
   },
   "outputs": [],
   "source": [
    "# Manually preprocessing image\n",
    "preprocessed_image = original_image.resize((256, 256))\n",
    "preprocessed_image = np.array(preprocessed_image) / 255.0\n",
    "\n",
    "preds = model.predict(np.expand_dims(preprocessed_image, axis = 0))\n",
    "labels = ['Healthy', 'Powdery', 'Rust']\n",
    "\n",
    "preds_class = np.argmax(preds)\n",
    "preds_label = labels[preds_class]\n",
    "\n",
    "print(f'\\nPredicted Class: {preds_label}')\n",
    "print(f'\\nConfidence Score: {preds[0][preds_class]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8HVuAgu-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The model is 100% certain that the plant in the picture belongs to the <i><b>Rust</b></i> class, which is also correct.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T14:03:31.838421Z",
     "iopub.status.busy": "2023-10-15T14:03:31.837983Z",
     "iopub.status.idle": "2023-10-15T14:03:32.47319Z",
     "shell.execute_reply": "2023-10-15T14:03:32.472175Z",
     "shell.execute_reply.started": "2023-10-15T14:03:31.838389Z"
    },
    "id": "5zdV_FOi-Nlg"
   },
   "outputs": [],
   "source": [
    "# Loading an image from the Validation/ Healthy directory\n",
    "image_path = '/kaggle/input/plant-disease-recognition-dataset/Validation/Validation/Healthy/9c99786a63786571.jpg'\n",
    "original_image = Image.open(image_path)\n",
    "og_width, og_height = original_image.size\n",
    "\n",
    "# Resizing image for optimal performance\n",
    "new_width = int(og_width * .20) # 20% of the original size\n",
    "new_height = int(og_height * .20) # 20% of the original size\n",
    "\n",
    "resized_img = original_image.resize((new_width, new_height))\n",
    "print('Picture of a Healthy Plant: \\n')\n",
    "resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T14:03:37.686834Z",
     "iopub.status.busy": "2023-10-15T14:03:37.686517Z",
     "iopub.status.idle": "2023-10-15T14:03:37.897182Z",
     "shell.execute_reply": "2023-10-15T14:03:37.896222Z",
     "shell.execute_reply.started": "2023-10-15T14:03:37.686809Z"
    },
    "id": "-macHyjS-Nlg"
   },
   "outputs": [],
   "source": [
    "# Manually preprocessing image\n",
    "preprocessed_image = original_image.resize((256, 256))\n",
    "preprocessed_image = np.array(preprocessed_image) / 255.0\n",
    "\n",
    "preds = model.predict(np.expand_dims(preprocessed_image, axis = 0))\n",
    "labels = ['Healthy', 'Powdery', 'Rust']\n",
    "\n",
    "preds_class = np.argmax(preds)\n",
    "preds_label = labels[preds_class]\n",
    "\n",
    "print(f'\\nPredicted Class: {preds_label}')\n",
    "print(f'\\nConfidence Score: {preds[0][preds_class]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUtjKczl-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">The model is 100% certain that the plant in the picture belongs to the <i><b>Healthy</b></i> class, which is also correct.</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy9mTxqC-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">After running several tests with other pictures, I could identify that the current model is performing fairly well in classifying all the three classes.</p>  \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">To save the current weights, so you can deploy this model or continue working with it later on, you can simply use Keras' <code>.save()</code> method. This is going to save your model as an HDF5 file.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T13:56:40.931608Z",
     "iopub.status.busy": "2023-10-15T13:56:40.931288Z",
     "iopub.status.idle": "2023-10-15T13:56:42.478677Z",
     "shell.execute_reply": "2023-10-15T13:56:42.477715Z",
     "shell.execute_reply.started": "2023-10-15T13:56:40.931578Z"
    },
    "id": "-73LSw2k-Nlg"
   },
   "outputs": [],
   "source": [
    "model.save('plant_disease_classifier.h5') # Saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beXqbcOX-Nlg"
   },
   "source": [
    "<div id = 'conclusion'\n",
    "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 2.85px solid orange;\n",
    "               width: 100%;\n",
    "               margin-top: 62px;\n",
    "               margin-bottom: auto;\n",
    "               margin-left: 0;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 2.25px;\"><b>Conclusion</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDAnVpMy-Nlg"
   },
   "source": [
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">In this notebook, we explored the basics of Convolutional Neural Networks. We delved deeper into the main layers â€” Convolutional, Pooling, etc. â€”, activation functions, as well as many other techniques to work with image data and CNNs for image classification.</p>  \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Even though many tasks nowadays can be efficiently done with pre-trained models, that can be easily accessible via platforms such as TensorFlow Hub and HuggingFace, it is still essential to understand what is the role of each layer inside a Convolutional Neural Network and how they interact with each other. This is why this notebook have the intention of guiding you through the process of building a CNN from scratch, and I plan to bring more notebooks such as this one for other <i>Deep Learning</i> tasks and architectures. </p>  \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Our model scored 97.0% in accuracy while predicting labels for the validation dataset, which is a great performance, and it was competent to identify relevant patterns across all the classes in the dataset.</p>  \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">I hope that this notebook serves as an introduction to those that are still just starting to explore ConvNets, or even help veterans to refine their knowledge on some of the basics. Please, feel free to copy this notebook and edit it as you wish, specially to try your own improvements for higher performance and testings.</p>  \n",
    "          \n",
    "<p style=\"font-family: Calibri, serif; text-align: left;\n",
    "          font-size: 24px; letter-spacing: .85px;\">Thank you so much for reading. Your feedback, upvotes, and suggestions are always much welcome!</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy041nqR-Nlh"
   },
   "source": [
    "<hr style=\"border: 0;\n",
    "           height: 1px;\n",
    "           border-top: 0.85px;\n",
    "           solid #b2b2b2\">\n",
    "           \n",
    "<div style=\"text-align: left;\n",
    "            color: #8d8d8d;\n",
    "            padding-left: 15px;\n",
    "            font-size: 14.25px;\">\n",
    "    Luis Fernando Torres, 2023 <br><br>\n",
    "    Let's connect!ðŸ”—<br>\n",
    "    <a href=\"https://www.linkedin.com/in/luuisotorres/\">LinkedIn</a> â€¢ <a href=\"https://medium.com/@luuisotorres\">Medium</a> â€¢ <a href = \"https://huggingface.co/luisotorres\">Hugging Face</a><br><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ðŸ§  Convolutional Neural Network From Scratch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MLExp",
   "language": "python",
   "name": "mlexp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
